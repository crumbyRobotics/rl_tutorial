{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array').unwrapped\n",
    "\n",
    "# matplotlibの設定\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# gpuが使用される場合の設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity # サイクルバッファ\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr=0.003):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.pi_mean = nn.Linear(64, action_dim)\n",
    "        self.pi_stddev = nn.Linear(64, action_dim)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        mean = F.linear(self.pi_mean(x))\n",
    "        stddev = F.linear(self.pi_stddev(x))\n",
    "\n",
    "        stddev = torch.exp(stddev)\n",
    "\n",
    "        return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr=0.003):\n",
    "        super(DualQNetwork, self).__init__()\n",
    "        # QNetwork 1\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.q1 = nn.Linear(64, 1)\n",
    "        # QNetwork 2\n",
    "        self.layer4 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 64)\n",
    "        self.q2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat((s, a), -1) # combination s and a\n",
    "  \n",
    "        # QNetwork 1\n",
    "        x1 = F.relu(self.layer1(x))\n",
    "        x1 = F.relu(self.layer2(x1))\n",
    "        x1 = F.relu(self.layer3(x1))\n",
    "        q_value1 = F.linear(self.q1(x1))\n",
    "        # QNetwork 2\n",
    "        x2 = F.relu(self.layer1(x))\n",
    "        x2 = F.relu(self.layer2(x2))\n",
    "        x2 = F.relu(self.layer3(x2))\n",
    "        q_value2 = F.linear(self.q2(x2))\n",
    "\n",
    "        return q_value1, q_value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, state_space, action_space, buffer_size, gamma, soft_target_tau, hard_target_interval, \n",
    "                 target_entropy, policy_lr, q_lr, alpha_lr):\n",
    "        super(SAC, self).__init__()\n",
    "\n",
    "        self.state_dim = state_space.shape[0]\n",
    "        self.action_dim = action_space.shape[0]\n",
    "\n",
    "        # Envアクション用にスケールする\n",
    "        self.action_center = (action_space.high + action_space.low) / 2\n",
    "        self.action_scale = action_space.high - self.action_center\n",
    "\n",
    "        # Neural Networks\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, policy_lr)\n",
    "        \n",
    "        self.q_net = DualQNetwork(self.state_dim, self.action_dim, q_lr)\n",
    "        self.target_q_net = DualQNetwork(self.state_dim, self.action_dim, q_lr)\n",
    "\n",
    "        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.replay_memory = ReplayMemory(buffer_size)\n",
    "\n",
    "        \n",
    "        self.target_entropy = -self.action_dim\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "\n",
    "\n",
    "        # Hyper Parameters\n",
    "        self.gamma = gamma\n",
    "        self.log_alpha = 0\n",
    "        self.soft_target_tau = soft_target_tau\n",
    "        self.hard_target_interval = hard_target_interval\n",
    "        self.target_entropy = target_entropy\n",
    "\n",
    "        self.train_count = 0\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mean, stddev = self.policy_net(state)\n",
    "\n",
    "        # Reparameterization\n",
    "        normal_random = torch.normal(0, 1, size=mean.shape)\n",
    "        action_org = mean + stddev * normal_random\n",
    "\n",
    "        # Squashed Gaussian Policy\n",
    "        action = torch.tanh(action_org)\n",
    "\n",
    "        return action, mean, stddev, action_org\n",
    "\n",
    "    def scaled_sample_action(self, state):\n",
    "        action, _, _, _ = self.sample_action(state)\n",
    "        env_action = action.numpy()[0] * self.action_scale + self.action_center\n",
    "\n",
    "        return env_action, action\n",
    "\n",
    "    # 正規分布でのactionの対数確率密度関数logμ(a|s)\n",
    "    def compute_logpi(self, mean, stddev, action):\n",
    "        a1 = -0.5 * np.log(2*np.pi)\n",
    "        a2 = -torch.log(stddev)\n",
    "        a3 = -0.5 * (((action - mean) / stddev) ** 2)\n",
    "        return a1 + a2 + a3\n",
    "\n",
    "    # tanhで変換されたactionのlogπ(a|s)をaction_orgを使って計算\n",
    "    def compute_logpi_sgp(self, mean, stddev, action_org):\n",
    "        logmu = self.compute_logpi(mean, stddev, action_org)\n",
    "        tmp = 1 - torch.tanh(action_org) ** 2\n",
    "        tmp = torch.clip(tmp, 1e-10, 1.0)  # log(0)回避\n",
    "        logpi = logmu - torch.sum(torch.log(tmp), 1, keepdim=True)\n",
    "        return logpi\n",
    "\n",
    "\n",
    "    def update(self, batch_size, q_net_sync=False):\n",
    "        # 経験をバッチでサンプリング\n",
    "        state_batch, action_batch, n_state_batch, reward_batch, done_batch = self.replay_memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        n_state_batch = torch.FloatTensor(n_state_batch)\n",
    "        action_batch = torch.FloatTensor(action_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "        done_batch = torch.BoolTensor(done_batch)\n",
    "\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        \n",
    "        # Q(s,a)の推定値を計算し, Q値の損失関数を計算\n",
    "        with torch.no_grad():\n",
    "            n_action, n_mean, n_stddev, n_action_org = self.policy_net(n_state_batch)\n",
    "            \n",
    "            n_logpi = self.compute_logpi_sgp(n_mean, n_stddev, n_action_org)\n",
    "            n_q1, n_q2 = self.target_q_net(n_state_batch, n_action)\n",
    "\n",
    "            q_est = reward_batch + (1 - done_batch) * self.gamma * torch.minimum(n_q1, n_q2) - (alpha * n_logpi)\n",
    "        q1, q2 = self.q_net(state_batch, action_batch)\n",
    "        q1_loss = F.mse_loss(q1, q_est)\n",
    "        q2_loss = F.mse_loss(q2, q_est)\n",
    "        q_loss = q1_loss + q2_loss\n",
    "\n",
    "        # q_lossからQNetworkを学習\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_net.optimizer.step()\n",
    "\n",
    "        # 方策の損失関数を計算\n",
    "        action, mean, stddev, action_org = self.sample_action(state_batch) # 現在の方策π(θ)で選ばれるactionについて評価       \n",
    "        logpi = self.compute_logpi_sgp(mean, stddev, action_org)\n",
    "        q1, q2 = self.q_net(state_batch, action)\n",
    "        q_min = torch.minimum(q1, q2)\n",
    "        policy_loss =  torch.mean(-q_min + alpha * logpi)\n",
    "\n",
    "        # policy_lossからPolicyNetworkを学習\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "\n",
    "        # αの自動調整\n",
    "        alpha_loss = torch.mean(-self.log_alpha * (logpi + self.target_entropy))\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        # ソフトターゲットで更新\n",
    "        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.soft_target_tau) + param.data * self.soft_target_tau)\n",
    "\n",
    "        # q_net_syncフラグが有効ならq_netを同期させる\n",
    "        if q_net_sync:\n",
    "            for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "                target_param.data.copy_(param.data)\n",
    "\n",
    "        return policy_loss, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
