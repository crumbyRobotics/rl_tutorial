{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9d2f73d6a0>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from cmath import isnan\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array').unwrapped\n",
    "\n",
    "# matplotlibの設定\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# gpuが使用される場合の設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity # サイクルバッファ\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr=0.003):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.pi_mean = nn.Linear(64, action_dim)\n",
    "        self.pi_stddev = nn.Linear(64, action_dim)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        stddev = torch.exp(stddev)\n",
    "\n",
    "        return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr=0.003):\n",
    "        super(DualQNetwork, self).__init__()\n",
    "        # QNetwork 1\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.q1 = nn.Linear(64, 1)\n",
    "        # QNetwork 2\n",
    "        self.layer4 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 64)\n",
    "        self.q2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat((s, a), 1) # combination s and a\n",
    "        # QNetwork 1\n",
    "        x1 = F.relu(self.layer1(x))\n",
    "        x1 = F.relu(self.layer2(x1))\n",
    "        x1 = F.relu(self.layer3(x1))\n",
    "        x1 = self.q1(x1)\n",
    "        # QNetwork 2\n",
    "        x2 = F.relu(self.layer4(x))\n",
    "        x2 = F.relu(self.layer5(x2))\n",
    "        x2 = F.relu(self.layer6(x2))\n",
    "        x2 = self.q2(x2)\n",
    "\n",
    "        return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, state_space, action_space, buffer_size, gamma, soft_target_tau, hard_target_interval, \n",
    "                 target_entropy, policy_lr, q_lr, alpha_lr):\n",
    "        super(SAC, self).__init__()\n",
    "\n",
    "        self.state_dim = state_space.shape[0]\n",
    "        self.action_dim = action_space.shape[0]\n",
    "\n",
    "        # Envアクション用にスケールする\n",
    "        self.action_center = torch.FloatTensor((action_space.high + action_space.low) / 2)\n",
    "        self.action_scale = torch.FloatTensor(action_space.high - self.action_center.detach().numpy())\n",
    "\n",
    "        # Neural Networks\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, policy_lr)\n",
    "        \n",
    "        self.q_net = DualQNetwork(self.state_dim, self.action_dim, q_lr)\n",
    "        self.target_q_net = DualQNetwork(self.state_dim, self.action_dim, q_lr)\n",
    "\n",
    "        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.replay_memory = ReplayMemory(buffer_size)\n",
    "\n",
    "        \n",
    "        self.target_entropy = -self.action_dim\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "\n",
    "        # Hyper Parameters\n",
    "        self.gamma = gamma\n",
    "        self.soft_target_tau = soft_target_tau\n",
    "        self.target_entropy = target_entropy\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mean, stddev = self.policy_net(state)\n",
    "\n",
    "        # Reparameterization\n",
    "        normal_random = torch.normal(0, 1, size=mean.shape)\n",
    "        action_org = mean + stddev * normal_random\n",
    "\n",
    "        # Squashed Gaussian Policy\n",
    "        action = torch.tanh(action_org)\n",
    "\n",
    "        return action, mean, stddev, action_org\n",
    "\n",
    "    def scaled_sample_action(self, state):\n",
    "        action, _, _, _ = self.sample_action(state)\n",
    "        env_action = action * self.action_scale + self.action_center\n",
    "\n",
    "        return env_action, action\n",
    "\n",
    "    # 正規分布でのactionの対数確率密度関数logμ(a|s)\n",
    "    def compute_logpi(self, mean, stddev, action):\n",
    "        a1 = -0.5 * np.log(2*np.pi)\n",
    "        a2 = -torch.log(stddev)\n",
    "        a3 = -0.5 * (((action - mean) / stddev) ** 2)\n",
    "        return a1 + a2 + a3\n",
    "\n",
    "    # tanhで変換されたactionのlogπ(a|s)をaction_orgを使って計算\n",
    "    def compute_logpi_sgp(self, mean, stddev, action_org):\n",
    "        logmu = self.compute_logpi(mean, stddev, action_org)\n",
    "        tmp = 1 - torch.tanh(action_org) ** 2\n",
    "        tmp = torch.clip(tmp, 1e-10, 1.0)  # log(0)回避\n",
    "        logpi = logmu - torch.sum(torch.log(tmp), 1, keepdim=True)\n",
    "        return logpi\n",
    "\n",
    "\n",
    "    def update(self, batch_size, q_net_sync=False):\n",
    "        # 経験をバッチでサンプリング\n",
    "        transitions = self.replay_memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        n_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.cat(batch.done)\n",
    "\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        \n",
    "        # Q(s,a)の推定値を計算し, Q値の損失関数を計算\n",
    "        with torch.no_grad():\n",
    "            n_action, n_mean, n_stddev, n_action_org = self.sample_action(n_state_batch)\n",
    "            \n",
    "            n_logpi = self.compute_logpi_sgp(n_mean, n_stddev, n_action_org)\n",
    "            n_q1, n_q2 = self.target_q_net(n_state_batch, n_action)\n",
    "          \n",
    "            q_est = reward_batch + (1 - done_batch) * self.gamma * torch.minimum(n_q1, n_q2) - (alpha * n_logpi)\n",
    "        q1, q2 = self.q_net(state_batch, action_batch)\n",
    "        q1_loss = F.mse_loss(q1.float(), q_est.float())\n",
    "        q2_loss = F.mse_loss(q2.float(), q_est.float())\n",
    "        q_loss = q1_loss + q2_loss\n",
    "        \n",
    "        # q_lossからQNetworkを学習\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        print(\"aaaaaaaaa\", q_loss.backward())\n",
    "        self.q_net.optimizer.step()\n",
    "\n",
    "        # 方策の損失関数を計算\n",
    "        action, mean, stddev, action_org = self.sample_action(state_batch) # 現在の方策π(θ)で選ばれるactionについて評価     \n",
    "        logpi = self.compute_logpi_sgp(mean, stddev, action_org)\n",
    "        q1, q2 = self.q_net(state_batch, action)\n",
    "        q_min = torch.minimum(q1, q2)\n",
    "        policy_loss =  (-q_min + alpha.detach() * logpi).mean()\n",
    "\n",
    "        # policy_lossからPolicyNetworkを学習\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "\n",
    "        # αの自動調整\n",
    "        alpha_loss = -(self.log_alpha * (logpi + self.target_entropy).detach()).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        # ソフトターゲットで更新\n",
    "        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.soft_target_tau) + param.data * self.soft_target_tau)\n",
    "\n",
    "        # q_net_syncフラグが有効ならq_netを同期させる\n",
    "        if q_net_sync:\n",
    "            for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "                target_param.data.copy_(param.data)\n",
    "\n",
    "        return policy_loss, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (min,ave,max)reward -767.3 -767.3 -767.3, alpha=1.000\n",
      "aaaaaaaaa None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [444], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     q_net_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# モデルの更新\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m metrics \u001b[39m=\u001b[39m sac\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m     67\u001b[0m     batch_size,\n\u001b[1;32m     68\u001b[0m     q_net_sync)\n\u001b[1;32m     69\u001b[0m train_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     70\u001b[0m metrics_list\u001b[39m.\u001b[39mappend(metrics)\n",
      "Cell \u001b[0;32mIn [443], line 96\u001b[0m, in \u001b[0;36mSAC.update\u001b[0;34m(self, batch_size, q_net_sync)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m# q_lossからQNetworkを学習\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maaaaaaaaa\u001b[39m\u001b[39m\"\u001b[39m, q_loss\u001b[39m.\u001b[39;49mbackward())\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m \u001b[39m# 方策の損失関数を計算\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "# ハイパーパラメータ\n",
    "buffer_size = 1000  # Experienceのキュー容量\n",
    "warmup_size = 500  # 学習するかどうかのExperienceの最低限の容量\n",
    "train_interval = 10  # 学習する制御周期間隔\n",
    "batch_size = 32  # バッチサイズ\n",
    "gamma = 0.9  # 割引率\n",
    "soft_target_tau = 0.02  # Soft TargetでTargetに近づく割合\n",
    "hard_target_interval = 100  # Hard Targetで同期する間隔\n",
    "lr = 0.003\n",
    "# エントロピーαの目標値: -1xアクション数がいいらしい\n",
    "target_entropy = -1 * env.action_space.shape[0]\n",
    "\n",
    "sac = SAC(env.observation_space, env.action_space, buffer_size, gamma, soft_target_tau, hard_target_interval,\n",
    "            target_entropy, lr, lr, lr)\n",
    "\n",
    "step_count = 0\n",
    "train_count = 0\n",
    "\n",
    "# 記録用\n",
    "history_rewards = []\n",
    "history_metrics = []\n",
    "history_metrics_y = []\n",
    "\n",
    "\n",
    "# 学習ループ\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    # １エピソード\n",
    "    while not done:\n",
    "        # アクションを決定\n",
    "        env_action, action = sac.scaled_sample_action(torch.FloatTensor(state).unsqueeze(0))\n",
    "        if isnan(env_action[0]):\n",
    "            print(\"action is NaN. 学習失敗.\")\n",
    "            break\n",
    "        # print(\"state:\", state, \"action:\", action)\n",
    "\n",
    "        n_state, reward, terminated, truncated, _ = env.step(env_action.detach().numpy()[0])\n",
    "        n_state = np.asarray(n_state)\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        sac.replay_memory.push(\n",
    "            torch.tensor(state).reshape(1, -1),\n",
    "            action,\n",
    "            torch.tensor(n_state).reshape(1, -1),\n",
    "            torch.tensor(reward).reshape(1, -1), \n",
    "            torch.tensor(done).reshape(1, -1).int())\n",
    "\n",
    "        state = n_state\n",
    "\n",
    "        # train_interval毎に, warmup貯まっていたら学習する\n",
    "        if len(sac.replay_memory) >= warmup_size and step_count % train_interval == 0:\n",
    "            q_net_sync = False\n",
    "            if train_count % hard_target_interval == 0:\n",
    "                q_net_sync = True\n",
    "            # モデルの更新\n",
    "            metrics = sac.update(\n",
    "                batch_size,\n",
    "                q_net_sync)\n",
    "            train_count += 1\n",
    "            metrics_list.append(metrics)\n",
    "        step_count += 1\n",
    "\n",
    "    # 報酬\n",
    "    history_rewards.append(total_reward)\n",
    "\n",
    "    # メトリクス\n",
    "    if len(metrics_list) > 0:\n",
    "        history_metrics.append(np.mean(metrics_list, axis=0))  # 平均を保存\n",
    "        history_metrics_y.append(episode)\n",
    "\n",
    "    #--- print\n",
    "    interval = 20\n",
    "    if episode % interval == 0:\n",
    "        print(\"{} (min,ave,max)reward {:.1f} {:.1f} {:.1f}, alpha={:.3f}\".format(\n",
    "            episode,\n",
    "            min(history_rewards[-interval:]),\n",
    "            np.mean(history_rewards[-interval:]),\n",
    "            max(history_rewards[-interval:]),\n",
    "            torch.exp(sac.log_alpha).detach().numpy()[0],\n",
    "        ))\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
